\documentclass[10pt]{article}
\usepackage[compact,small]{titlesec} % makes compact or small section headers
\usepackage[comma,authoryear]{natbib}
\usepackage{graphicx}
\usepackage{amssymb,amsmath}
\usepackage{latexsym}
\usepackage{pst-node,pst-tree}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{caption}

\usepackage{xspace}
\input{mmp-commands}

% Lengths and formatting

\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\topmargin}{-1.5cm}
\setlength{\textheight}{9in}
\setlength{\textwidth}{6.5in}
\setlength{\parskip}{.1 em}
%\setlength{\parindent}{0em}
\setlength{\itemsep}{0em}
\setlength{\tabcolsep}{0em}

% Run in section headings to save space
\titleformat{\section}[block]
  {\large\bfseries\filright}
  {\thesection}{6pt}{}
\titlespacing{\section}
  {\parindent}{*2}{\wordsep}

\titleformat{\subsection}[block]
%\titleformat{\subsection}[runin]
  {\normalfont\bfseries}
  {\thesubsection}{6pt}{}
\titlespacing{\subsection}
  {\parindent}{*2}{\wordsep}

\titleformat{\subsubsection}[runin]
  {\normalfont\bfseries}
  {\thesubsubsection}{6pt}{}


\newtheorem{prop}{Proposition}
\newtheorem{definition}[prop]{Definition}
\newtheorem{remark}[prop]{Remark}

%James added commands below
\newcommand\bl{\left(}
\newcommand\br{\right)}



\newlength{\picwi}

\title{NSASAG Problem 23-02 Comparing ranked lists}
\author{Marina Meil\u{a} and James Buenfil\\{\tt mmp@stat.washington.edu}}


\date{March 1, 2024}

\begin{document}

\maketitle

%\mmp{This is a preliminary DRAFT. In addition to the models listed here, several models will be reported on by Yikun Zhang.}

\section{Notation}
\label{sec:notation}

Let $\pi=(x_1,\ldots x_n)$, $\pi'=(y_1,\ldots y_m)$ be the two lists; we will call lists like these \topt permutations; here $t=n$, respectively $m$. We may denote alternatively $x_j\equiv \pi_j, y_j\equiv \pi'_j$, and $|\pi|=n,|\pi'|=m$; then $\pi_{1:k}\equiv x_{1:k}$ is a prefix of $\pi$.% If necessary, we assume $n\geq m$. 

We will assume that the lists $\pi,\pi'$ are drawn from the same probability
distribution $\probb$ over a very large set of items $\X$. The modal
 (most probable, ``true'') permutation of $\probb$ is
denoted by $\sigma_0$. In general, we denote by $\sigma\in \ssss_{\X}$
complete permutations of the elements of $\X$; we also denote with $\sigma$ complete or truncated ``model parameters'', while with $\pi$ we denote observations (always truncated). We will never have access to the entire $\X,\sigzero$ or $\probb$, but we will assume these exist and do math with them. In fact, in some cases, we can assume $\X$ is countable. In general, it will be unreasonable to obtain results that depend strongly on $|\X|$. 
 In this report, ranking, list, are used interchangeably; for permutations of the entire set $\X$ we may also use the term permutation.

In the sections below, we consider several possible model families for $\probb$. The approach described here is with the first family, \gmms, in mind, but for obvious reasons it would apply to other model familes.


\section{Approach}
\label{sec:approach}
Desired properties/constraints of the solution
\benum
\item $\pi,\pi'$ do not have the same elements; $m\neq n$
\item $\pi \cup \pi'$ too large to handle or sort; we can't assume to know $\probb(\pi \cup \pi')$
\item We only vaguely know the size of $\X$, results should not depend strongly on it. A note on terminology: standard models over rankings assume that $\X$ is ``known." For simplicity, I will call this case (abusively) {\em FINITE} $\X$, and will bundle a very large incompletely known $\X$ with the {\em INFINITE} $\X$.
\item We cannot get more samples from $\probb$
\item The null hypothesis is that the lists are sampled from the same distribution.
\item We assume that the recommender system (i.e. $\probb$) is good enough, hence the lists $\pi,\pi'$ are ``close''. Implicitly we assume also that the recommender system is non-deterministic, i.e. it can give slightly varying answers to the same query.  
  \eenum
  %
Thus, as the proposer also implies, a general strategy will be to measure a distance $d(\pi,\pi')$ and to reject the null when the distance is too large to be generated from a $\probb$ which is concentrated around its mode $\sigzero$. 
  %TODO: keep this line?

\section{Infinite Generalized Mallows S models (\gmms)}
\label{sec:gmms}

The \gmms~model is described here briefly; the details are to be found in the reference
\cite{MBao}.

\benum
\item In \gmms, we assume that $\X$ is countable. A finite version exists and all the results in \cite{MBao} transfer to the finite models with minor changes.
\item \gmms~has 2 parameters: the {\em modal permutation} $\sigma_0\in
  \ssss_{\X}$, and a {\em parameter vector} 
  $p=(p_1,p_2,\ldots p_j,\ldots)$, where
  $p_j\in (0,1)$ is a {\em concentration} parameter associated
  with rank $j$ (more details below). Note that $p$ is an
  infinite vector, $\sigma_0$ an infinite permutation, and the samples
  from $\probb=\gmms_{\sigzero,p}$ are infinite permutations as
  well. Let this not trouble us.
\item We assume that all we observe from the samples drawn from $\gmms_{\sigzero,p}$ are \topt permutations (denoted by $\pi,\pi',\ldots$) truncated at arbitrary values $t$. Since $t$ is observed, we don't need to learn anything about it.
\item
  In this case, the beauty of $\gmms$ is that $\probb(\pi)$, where $\pi$ is a \topt permutation, depends only on $p_{1:t}$ and a {\em finite prefix} of $\sigzero$. In \cite{MBao} it is shown how the \gmms~can be estimated by Maximum Likelihood from a sample of \topt permutations. That is, any finite data set will determine finite prefixes of $\sigzero$ and $p$ and leave the rest completely undetermined.
\item The \gmms~is an exponential family model given by
  \beq
\gmms(\pi|\sigzero,p)\;=\;\prod_{j=1}^t\frac{1}{Z(p_j)}p_j^{s_j},
\eeq
%
where $t=|\pi|$, and $s_{1:t}\in\nnnn$ are an alternative representation of $\pi$ known as the {\em code} of $\pi$ \cite{Stanley} (or inversion table). The normalization constants are the sum of the infinite geometric series $1+p_j+p_j^2+\ldots$, i.e.
\beq
Z(p)\;=\;\frac{1}{1-p}.
\eeq
For infinite $\X$, this model is equivalent to saying that each $s_j$ for $j=1 ,\ldots t$ is sampled independently from a geometric distribution with parameter $1-p_j$. We use the convention that the geometric distribution counts the number of failures before the first success in a sequence of Bernoulli trials, where the probability of success is $1-p_j$.
%TODO: I removed this line: For finite $\X$, $Z$ becomes the sum of a {\em finite} geometric series.
\item The \gmms~has sufficient statistics (finite if data is finite), which consist of counts $Q_{j,ab}=\#\{(a\prec b) \text{and} \rank(a)=j\}$ in sample.
\item In addition, from \cite{MBao} we know that for given $\sigzero$, the conjugate prior over $p_j$ is a Beta distribution (naturally, given that \eqref{eq:gmms} is an exponential family). 
\eenum
So, the advantages of the \gmms~are that its stagewise data generation models fits with the assumptions of our problem, and that we understand their properties pretty well, relative to other $\probb$'s over $\ssss_{\X}$.

In particular, if we want to model less variability in the top ranks than at the middle or bottom ranks, we can set smaller $p_j$ values for the higher ranks. A proposal is to have slowly decay as $p_j\propto a^{1/j}$. %TODO: supposed to be a reference here?

\subsection{Estimate $\gmms$ from $\pi,\pi'$}
\label{sec:gmm-sig0}
\benum
\item \label{it:est-sig0}  Estimate a model $\sigzero,p_{1:n}$ from $\pi,\pi'$ by ML. Of course this will be a very rough estimate and we will have to consider this in the ``confidence'' of our inferences.

  Regardless of the statistical confidence, with a sample of 2, there will be multiple (many!) equally likely modal permutation $\sigzero$. This is not bad, because we would have to smooth/average over the most likely $\sigzero$'s anyways (in a Bayesian fashion or not).

Suppose we get a set $S$ of $\sigma$ candidates for modal permutations (note that these, being estimates, are all truncated to some finite length).

\item \label{it:est-thet} For each $\sigma\in S$, we estimate $\hat{p}(\sigma)$ the ML parameters (straightforward, again with high variance). We can also get the MAP estimate or posterior from the conjugate prior (in closed form, given $\sigma$).

\item \label{it:threshold} A $\gmms$ is more dispersed when $p_j\approx 1$ and is highly concentrated when $p_j\approx 0$. If the expected $p_j$'s estimated above are above a threshold, it will mean that the recommender system is not very good. If we believe we have a good recommender system, then we should reject the null.  
\eenum

Pros:
\bit
\item The part on $\sigma$ estimation is straightforward.
  \item Steps \ref{it:est-sig0}, \ref{it:est-thet} can be done by
  Monte Carlo \cite{ChenM}\mmp{to check!} with a conjugate prior
  non-informative on $\sigzero$. %TODO: keep this part?
\item Many   reasonable adaptations to Step \ref{it:threshold} can be
  designed.
  \eit
  
Cons: There can be very many $\sigma$'s in $S$. $\sigma$ will have to be over $\pi\cup \pi'$; is this a problem?

\subsection{Sampling from the null, without committing to $\sigzero$}
\label{sec:gmm-sampling}
It could be more truthful to the problem to {\em not} attempt to estimate $\sigzero$ at all. The nice thing about a $\gmms$ is that the dispersion is invariant to the modal permutation; i.e. all the properties of the \gmms~hold w.l.o.g. for $\sigzero=\idperm$ the identity. That's the same as saying that items don't count, only their ranks and relative ranks.

The sampling algorithm to collect statistics on ranks, is very simple.

\benum
\item[]{\bf Algorithm} \algsamgmm
\item[Input] $t$ length of $\pi$,  parameters $p_{1:n}$.
\item[Sample $s_{1:t}$] For $j=1:t$, sample $s_j\sim \geom(1-p_j)$.
\item[Decode] Make a list $1:N$ with $N=t+\max s_j$; this is a prefix of $\sigzero$. Construct $\pi$ from $\sigzero$ and $(s_{1:t})$ according to the recursive algorithm in \cite{MBao}. %Obviously this step can be made more efficient in numerous ways. 
\item[Output] $\pi$, code $(s_{1:t})$
  \eenum

{\bf Variation} The $p$'s can be sampled fromt the Beta prior. This would be an informative prior, concentrating on small values for $p_j$ (i.e. we believe the recommender is not too noisy).

With \algsamgmm, we can sample pairs of lists with lengths $m,n$, and compute rank statistics. 


\subsection{Distances and weighted distances between ordered lists} 
\label{sec:distances}
This subsection relies heavily on material from \cite{MBao}. A paper in a different style that introduces these ideas is \cite{fligner:86}.

The codes $s_j$ count inversions between $\pi$ and the modal permutation $\sigzero$; more specifically, $s_j$ is the number of inversions introduced at rank $j$ in the \algsamgmm algorithm. Therefore
\beq
\sum_{j=1}^t s_j\ln\frac{1}{p_j}=d_p(\pi,\sigzero),
\eeq
%
is a reasonable ``distance''\footnote{For complete permutations and $p_j=$ constant, $d_p$ is a metric, proportional to the Kendall $\tau$. However, in our case it is not a metric.} Note also that $\ln\frac{1}{p_j}$ is the natural parameter of the \gmms, and it's worth giving it the notation
%
\beq
\theta_j\;=\;\ln\frac{1}{p_j}
\eeq
%
\begin{remark}
    In \cite{MBao}, rather than parameterizing the model by the $p_j$ used here, $\theta_j$ is used instead. (see equation (4) of \cite{MBao}).)
\end{remark}

However, what we would like is a similarly weighted distance between two ordered lists $\pi$ and $\pi'$, rather than between $\pi$ and the modal permutation $\sigzero$. To do this, we carefully count how many inversions between $\pi$ and $\pi'$ appear at each rank. To define inversions between $\pi$ and $\pi'$ at a specific rank $j$, first we will define the total number of inversions between them.

\begin{definition}
The total number of inversions, denoted $\inv\bl \pi, \pi' \br$, between $\pi,\pi'$ is the number of pairs $\{a,b\}\subset \pi \cup \pi'$ on whose order $\pi$ and $\pi'$ disagree. 
\end{definition}
\textbf{Example 1:}\\
\textit{The number of inversions between the rankings $\pi = (2,3,4)$ and $\pi' = (1,3,2,4)$ is 4, because $\pi$ and $\pi'$ disagree on the pairs of elements $\{ 1,2 \}$, $\{ 1,3 \}$, $\{1,4\}$, and $\{2,3\}$.}\\\\
Moving forward, it will make things easier to ``complete" both lists. The following proposition says that the total number of inversions before and after completing the lists remains the same.

\begin{prop}
    Complete $\pi,\pi'$ to $\tilpi,\tilpi'$ as follows: append to $\pi$ the ordered set of items in $\pi'$ which are not in $\pi$, i.e. $\tilpi=\pi|\pi'_{|\pi'\setminus\pi}$, and similarly for $\tilpi'$. Then $|\tilpi|=|\tilpi'|=|\pi \cap \pi'|$, and $\inv(\pi,\pi') = \inv \bl \tilpi,\tilpi' \br$.
\end{prop}

Therefore, from now on, w.l.o.g., we can consider that $\pi,\pi'$ are complete permuations over the same set of $n$ items.

Next we define the number of inversions between $\pi$ and $\pi'$ introduced at rank $j$. 

\begin{definition}
    Let $\pi_{1:j}$ denote the prefix of $\pi$ found by considering only the first $j$ items, and similarly for $\pi'$. Then for $j=1, \ldots n$, $\Delta_j$,the number of inversions introduced at rank $j$, is defined by:
    \begin{equation}
        \Delta_j \equiv \inv \bl \pi_{1:j},\pi'_{1:j}\br - \inv \bl \pi_{1:(j-1)},\pi'_{1:(j-1)}\br
    \end{equation}
    where the total number of inversions between empty lists is defined to be $0$.
    In other words, it is the number of inversions added to the total number of inversions between $\pi_{1:(j-1)}$ and $\pi'_{1:(j-1)}$ when adding the $j$th elements $\pi_j$ and $\pi'_j$ to their lists respectively.
\end{definition}

Note that from the definition, the total number of inversions between $\pi$ and $\pi'$ can be computed from the $\Delta_j$ as $\inv\bl \pi,\pi'\br=\sum_{j=1:n}\Delta_j$.


Hence, a weighted (non-metric) Kendall-tau-like distance between $\pi,\pi'$ is
\beq
d_{K,p}(\pi,\pi')=\sum_{j=1}^{n} \theta_j \Delta_j
\eeq
This is a natural quantity around which we can define hypothesis tests to test if two given rankings came from the same distribution.

It turns out that we can compute the $\Delta_j$ using the formula in the following proposition.

%
\begin{prop} Denote $A_j = \pi_{1:j}$, $A'_j = \pi'_{1:j}$, $C_j=A_j\setminus A'_j$, and $C'_j=A'_j\setminus Aj$. Let $c_j(i')=\bl \text{rank of $i'$ in $C_j$}\br -1$, if $i'\in C_j$, and $|C_j|$ otherwise, and define $c'_j(i)$ similarly; let $i=\pi_{j+1},\,i'=\pi'_{j+1}$. Let I be the indicator for a logical statement ($1$ if true, $0$ if false). Then,
  \beq \label{eq:delta_formula} \Delta_{j+1}\,=\,c_j(i')+c'_j(i)+ I\bl i \neq i' \br I\bl i \notin C_j' \text{and } i \notin C_j \br.  \eeq
\end{prop}
%
    {\bf Proof} There are five cases. The proof follows from verifying the given formula in all cases:\\
    (i) $i,i'$ new, i.e. $i\not\in C'_j$, $i'\not\in C_j$, and $i=i'$. When $i$ is appended to $\pi$, all the items in $C'_j$ are yet to be appended to $\pi$, while they are already in $\pi'$. Therefore we introduce $c'_j(i)$ inversions. Similarly for $i'$.\\
    (ii) $i,i'$ new, and $i\neq i'$. This case is equivalent to case (i), except there is $1$ additional inversion added, since $i \neq i'$.\\
    (iii) $i\in C'_j,\,i'\not\in C_j$. For $i'$ which is new, the same reasoning as above gives us $c_j(i')$ inversions. For $i$ which is old, there are less inversions introduced, because all items which are ranked below $i$ do not add another inversion. Therefore, it is the rank of rank of $i$ in $C'_j$ minus $1$ which is the number of inversions added due to adding $i$ to $\pi_{1:j}$.\\
    (iv) $i\notin C'_j,\,i'\in C_j$.
    Analogous to case (iii).\\
    (v) $i\in C'_j,i'\in C_j$. Similar reasoning to the above. $\Box$.
    %
\begin{remark} 
Note that $\Delta_j$ is not equal to $s_j$, or $s'_j$, the codes of $\pi,\pi'$ w.r.t. $\sigma$. In fact, by the triangle inequality $\sum_j\Delta_j\leq \sum_j s_j +\sum_j s'_j$.  
\end{remark}


In practice, we compute $\Delta_{j+1}$ using the formula in the proposition (requiring $C_{j}$, $C_{j}'$, $\pi_{j+1}$, and $\pi_{j+1}'$). We can compute the $\Delta_j$ sequentially (and relatively quickly) thanks to the fact that we can find $C_{j+1}$ and $C_{j+1}'$ using only $C_{j}$, $C_{j}'$, $\pi_{j+1}$, and $\pi_{j+1}'$, and depending on which of the five cases we are in.\\\\
\textbf{Example 2}:\\
We look at the same rankings in Example 1. The completed rankings are $\pi = (2,3,4,1)$ and $\pi' = (1,3,2,4)$.
Computing $\Delta_1$:\\
$A_0 = A'_0 = (),$ $C_0 = C'_0 = ()$, the empty set. $i = \pi_1 = 2$, and $i'= \pi'_1 = 1$. We use equation \eqref{eq:delta_formula} which gives $\Delta_1 = 0 + 0 + 1 = 1$. The new pair where $\pi$ and $\pi'$ disagree is $\{ 1,2 \}$.\\\\
Computing $\Delta_2$:\\
$A_1 = (2)$, $A'_1 = (1)$, $C_1 = (2)$, $C'_1 = (1)$. $i = \pi_2 = 3$, and $i'= \pi'_1 = 3$. Equation \eqref{eq:delta_formula} gives $\Delta_2 = 1 + 1 + 0 = 2$. The new pairs where $\pi$ and $\pi'$ disagree are $\{ 1,3 \}$ and $\{2,3\}$.\\\\
Computing $\Delta_3$:\\
$A_2 = (2,3)$, $A'_2 = (1,3)$. $C_2 = (2)$, $C'_2 = (1)$. $i = \pi_3 = 4$, and $i'= \pi'_3 = 2$. Equation \eqref{eq:delta_formula} gives $\Delta_3 = 0 + 1 + 0 = 1$. The new pair where $\pi$ and $\pi'$ disagree is $\{1,4\}$\\\\
Computing $\Delta_4$:\\
$A_3 = (2,3,4)$, $A'_3 = (1,3,2)$. $C_3 = (4)$, $C'_3 = (1)$. $i = \pi_4 = 4$, and $i'= \pi'_4 = 1$. Equation \eqref{eq:delta_formula} gives $\Delta_3 = 0 + 0 + 0 = 0$.\\\\
We see that $\sum_{j=1}^4\Delta_j =\inv(\pi,\pi') = 4$, the total number of inversions between $\pi$ and $\pi'$.
\subsection{Accompanying code}
In the accompanying R code, we implement many of the ideas here, in particular:
\begin{enumerate}
    \item Maximum likelihood estimation of $\sigzero$ and $p_{1:n}$ from a finite sample of rankings discussed in Section \ref{sec:gmm-sig0}
    \item Sampling from the $\gmms$ model discussed in Section \ref{sec:gmm-sampling}.
    \item Computing the $\Delta_j$, the number of inversions between $\pi$ and $\pi'$ introduced at rank $j$ discussed in Section \ref{sec:distances}.
\end{enumerate}

We remark that in the code, we use the $\theta_j$ parameterization of the model, following \ref{MBao} $\bl \theta_j \equiv \ln{\frac{1}{p_j}} \br$.
\section{Stochastic Inversion Models}
\label{sec:stoch-i-m}
Widely analyzed theoretically, assumes independent inversions between each pair of items. Not a distribution over $\ssss_{\X}$; this is not necessarily a problem. Has a parameter per pair of items, handles partial comparisons data naturally (i.e. $a \prec b$), no explicit modal ranking (a plus). To see if it extends to very large $\X$. 




\section{Thurstonian models}
\label{sec:thurst}
In the Thurstonian model, each item is given a score $s_i = \mu_i + d_i$ where $\mu_i$ is the fixed mean of $s_i$ and $d_i$ is a random variable with mean $0$. Given $n$ total items, by putting the $d_i$ into a random vector $d \in \mathbb{R}^n$, a common approach is model their joint distribution as a multivariate normal with some covariance $\Sigma$. We could subsequently use maximum likelihood to estimate the means $\mu_i$ and the covariance matrix $\Sigma$. With only two observations, it would be required to place further modeling assumptions on $\mu$ and $\Sigma$, which could be appropriate (for example, supposing that $\Sigma = \sigma I_n$).

To sample from a Thurstonian model where we would like to incorporate that items could be missing, there are two obvious approaches. One is to first specify a length, sample the estimated model which includes all the items, and then to only keep the items that were ranked the highest (so we obtain a ranking of the specified length). The second approach is to specify a missingness mechanism (for example, each item has a parameter which determines the weighting of a Bernoulli random variable, and then the item is included in the model only if a Bernoulli variable for that item is a $1$ when sampled), and then sample the model restricted to the specified items. Although this second approach is natural, a disadvantage is that we no longer view the observed rankings as top $t$ rankings, instead we are looking at the ranks of the items being ranked relative to one another (e.g. if we observe a ranking with an item missing, we are not assuming that the missing item is ranked lower than the observed items, like we do in the Generalized Mallows Model).

An advantage of a Thurstonian model is that each item has a parameter, which is very intuitive. If we already have strong prior beliefs/ideas about what the parameters in a Thurstonian model are, then we could place a prior distribution on the parameters of the model directly, and then sample from it in order to obtain the distribution of test statistics.

%Top ranks have iid (?) distribution. Not known how far down this property holds.

\section{Plackett-Luce models}
\label{sec:pl}
The Plackett-Luce ranking model turns out to be special case of a Thurstonian model, where the $d_i$ are i.i.d. with a Gumbell distribution, but the means $\mu_i$ are allowed to be different (See Section 2.2 of \cite{guiver2009bayesian}). 
%Stagewise model, based on scores, great for \topt for finite $\X$. Not sure how to expand to very large $\X$. 
\section{Ranking with a $p$-norm push}
\label{sec:push}
To see if relevant.

\appendix




    
\end{document}